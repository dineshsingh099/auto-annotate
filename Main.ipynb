{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "from autodistill_grounding_dino import GroundingDINO\n",
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill_clip import CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(1)\n",
    "frame_width = video_capture.get(3)\n",
    "frame_height = video_capture.get(4)\n",
    "frame_size = (frame_width,frame_height)\n",
    "frame_rate = 15\n",
    "total_frames = 900\n",
    "frame_counter = 0\n",
    "video_writer = cv2.VideoWriter(filename=\"./video.mp4\",fourcc=cv2.VideoWriter_fourcc(*\"XVID\"),fps=frame_rate,\n",
    "                               frameSize=frame_size)\n",
    "\n",
    "while(frame_counter <= total_frames):\n",
    "\n",
    "    _, frame = video_capture.read()\n",
    "    video_writer.write(frame)\n",
    "    frame_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOS_DIR_PATH = \"/home/aiml_task/auto-annotate\"\n",
    "FRAMES_DIR_PATH = \"/home/aiml_task/auto-annotate/frames\"\n",
    "\n",
    "videos_path = sv.list_files_with_extensions(directory=VIDEOS_DIR_PATH, extensions=[\"mov\",\"mp4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in tqdm(videos_path):\n",
    "\n",
    "    video_name = str(video_path).split(\"/\")[-1]\n",
    "    img_file_name_pattern = video_name + \"-{:05d}.png\"\n",
    "\n",
    "    with sv.ImageSink(target_dir_path=FRAMES_DIR_PATH, image_name_pattern=img_file_name_pattern) as sink:\n",
    "        \n",
    "        for image in sv.get_video_frames_generator(source_path=str(video_path), stride=1):\n",
    "            sink.save_image(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = CaptionOntology({\"gestures made by fingers of hand \": \"hand\"})\n",
    "base_model = GroundingDINO(ontology=ontology)\n",
    "bbox_annotator = sv.BoxAnnotator()\n",
    "object_classes_to_detect = [\"hand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_detections = base_model.predict(\"./frames/video.mp4-00031.png\")\n",
    "bbox_coords = [np.uint16(bbox_coordinates) for bbox_coordinates, _, _, _, _, _ in model_detections][0]\n",
    "annotated_img = bbox_annotator.annotate(scene=plt.imread(\"./frames/video.mp4-00031.png\"),\n",
    "                                        detections=model_detections)\n",
    "\n",
    "sv.plot_image(annotated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread(\"./frames/video.mp4-00031.png\")[bbox_coords[1]:bbox_coords[3],bbox_coords[0]:bbox_coords[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"./detections\")\n",
    "detections_path = \"./detections\"\n",
    "\n",
    "for frame_path in pathlib.Path(\"./frames\").glob(\"*.png\"):\n",
    "\n",
    "    model_detections = base_model.predict(str(frame_path))\n",
    "    bbox_coords = [np.uint16(bbox_coordinates) for bbox_coordinates, _, _, _, _, _ in model_detections]\n",
    "\n",
    "    if len(bbox_coords) > 0:\n",
    "        bbox_coords = bbox_coords[0]\n",
    "        annotated_img = bbox_annotator.annotate(scene=plt.imread(str(frame_path)),\n",
    "                                        detections=model_detections)\n",
    "        plt.imsave(os.path.join(detections_path,\".\".join(str(frame_path).split(\"/\")[-1].split(\".\")[0:-1])+\".png\"),\n",
    "                   plt.imread(str(frame_path))[bbox_coords[1]:bbox_coords[3],bbox_coords[0]:bbox_coords[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_dict = dict()\n",
    "asl_characters = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "class_id2char = dict()\n",
    "\n",
    "for class_id, char in enumerate(asl_characters):\n",
    "\n",
    "    ontology_dict[f\"alphabet {char} in american sign language\"] = char\n",
    "    class_id2char[class_id] = char "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
